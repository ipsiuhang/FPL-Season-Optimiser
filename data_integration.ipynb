{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c49ba9",
   "metadata": {},
   "source": [
    "# Data Integration\n",
    "\n",
    "This notebook integrates two datasets from different sources (OLB and VAA) for FPL player data analysis.\n",
    "\n",
    "OLB: https://github.com/olbauday/FPL-Elo-Insights/tree/main/data/2024-2025\n",
    "\n",
    "VAA: https://github.com/vaastav/Fantasy-Premier-League/tree/master/data/2024-25\n",
    "\n",
    "RAN: https://github.com/Randdalf/fplcache/tree/main?tab=readme-ov-file  (honorable mention for understanding how the official fpl api cache works, and for cross referencing)\n",
    "## Technical Background\n",
    "\n",
    "- **Challenge**: No open dataset provides a complete time series of players' buying prices\n",
    "- **Player prices are dynamic**: Driven by transfer in/out (demand and supply)\n",
    "- **Price fluctuation**: Can rise or fall up to 0.1 Million per day\n",
    "\n",
    "**Study Period**: 2024-25 season, GW 1 to GW 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2370d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bcfc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Problem from olbauday (OLB Dataset)\n",
    "\n",
    "**Datasets used**: `players.csv`, `playerstats.csv`\n",
    "\n",
    "## Step 1: Map Player IDs to Names\n",
    "\n",
    "**Problem**: `players.csv` contains id-to-name mapping and `playerstats.csv` uses 'id' to identify players, but using player ID is not compatible with the vaastav dataset.\n",
    "\n",
    "**Solution**: Map the id to the player's name to make later merging easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07d7ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading players.csv...\n",
      "Found 804 distinct player IDs in players.csv\n",
      "Reading playerstats.csv...\n",
      "Found 804 distinct IDs in playerstats.csv\n",
      "\n",
      "Superset check PASSED - All IDs in playerstats.csv exist in players.csv\n",
      "Creating playerstats_mapped.csv...\n",
      "Successfully created gw_olb/playerstats_mapped.csv\n",
      "Total rows: 27657\n",
      "Columns: ['player_name', 'status', 'chance_of_playing_next_round', 'chance_of_playing_this_round', 'now_cost', 'now_cost_rank', 'now_cost_rank_type', 'cost_change_event', 'cost_change_event_fall', 'cost_change_start', 'cost_change_start_fall', 'selected_by_percent', 'selected_rank', 'selected_rank_type', 'total_points', 'event_points', 'points_per_game', 'points_per_game_rank', 'points_per_game_rank_type', 'bonus', 'bps', 'form', 'form_rank', 'form_rank_type', 'value_form', 'value_season', 'dreamteam_count', 'transfers_in', 'transfers_in_event', 'transfers_out', 'transfers_out_event', 'ep_next', 'ep_this', 'expected_goals', 'expected_assists', 'expected_goal_involvements', 'expected_goals_conceded', 'expected_goals_per_90', 'expected_assists_per_90', 'expected_goal_involvements_per_90', 'expected_goals_conceded_per_90', 'influence', 'influence_rank', 'influence_rank_type', 'creativity', 'creativity_rank', 'creativity_rank_type', 'threat', 'threat_rank', 'threat_rank_type', 'ict_index', 'ict_index_rank', 'ict_index_rank_type', 'corners_and_indirect_freekicks_order', 'direct_freekicks_order', 'penalties_order', 'gw', 'set_piece_threat']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read players.csv and create id-to-name mapping\n",
    "print(\"Reading players.csv...\")\n",
    "players_df = pd.read_csv('gw_olb/players.csv')\n",
    "\n",
    "# Create mapping dictionary: player_id -> \"first_name second_name\"\n",
    "players_df['full_name'] = players_df['first_name'] + ' ' + players_df['second_name']\n",
    "id_to_name = dict(zip(players_df['player_id'], players_df['full_name']))\n",
    "player_ids = set(players_df['player_id'].unique())\n",
    "print(f\"Found {len(player_ids)} distinct player IDs in players.csv\")\n",
    "\n",
    "# Step 2: Read playerstats.csv\n",
    "print(\"Reading playerstats.csv...\")\n",
    "playerstats_df = pd.read_csv('gw_olb/playerstats.csv')\n",
    "\n",
    "# Get distinct IDs from playerstats\n",
    "playerstats_ids = set(playerstats_df['id'].unique())\n",
    "print(f\"Found {len(playerstats_ids)} distinct IDs in playerstats.csv\")\n",
    "\n",
    "# Step 3: Check if player_ids is a superset of playerstats_ids\n",
    "is_superset = player_ids >= playerstats_ids\n",
    "\n",
    "if not is_superset:\n",
    "    # Find missing IDs\n",
    "    missing_ids = playerstats_ids - player_ids\n",
    "    print(f\"\\nERROR: Superset check FAILED!\")\n",
    "    print(f\"Found {len(missing_ids)} IDs in playerstats.csv that are not in players.csv:\")\n",
    "    for missing_id in sorted(missing_ids):\n",
    "        print(f\"  - ID: {missing_id}\")\n",
    "    print(\"\\nStopping script. Please provide instructions on how to proceed.\")\n",
    "else:\n",
    "    print(\"\\nSuperset check PASSED - All IDs in playerstats.csv exist in players.csv\")\n",
    "    \n",
    "    # Step 4: Create mapped dataframe\n",
    "    print(\"Creating playerstats_mapped.csv...\")\n",
    "    \n",
    "    # Map the 'id' column to 'player_name'\n",
    "    playerstats_df['player_name'] = playerstats_df['id'].map(id_to_name)\n",
    "    \n",
    "    # Drop the 'id' column and reorder to put player_name first\n",
    "    columns = ['player_name'] + [col for col in playerstats_df.columns if col not in ['id', 'player_name']]\n",
    "    mapped_df = playerstats_df[columns]\n",
    "    \n",
    "    # Save to new CSV\n",
    "    mapped_df.to_csv('gw_olb/playerstats_mapped.csv', index=False)\n",
    "    \n",
    "    print(f\"Successfully created gw_olb/playerstats_mapped.csv\")\n",
    "    print(f\"Total rows: {len(mapped_df)}\")\n",
    "    print(f\"Columns: {list(mapped_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d3477",
   "metadata": {},
   "source": [
    "## Step 2: Filter Playerstats Columns\n",
    "\n",
    "**Problem**: After mapping id to name, there are too many variables unnecessary for the study.\n",
    "\n",
    "**Solution**: Keep only the following variables: `player_name`, `status`, `chance_of_playing_next_round`, `chance_of_playing_this_round`, `now_cost`, `event_points`, `ep_next`, `ep_this`, `gw`\n",
    "\n",
    "**Important Note**: \n",
    "- The definition of 'total_points' in OLB's dataset is the **total points accumulated from GW1**\n",
    "- The definition of 'total_points' in VAA's dataset is the **total points accumulated in that GW**\n",
    "- After cross-referencing, 'event_points' in OLB's dataset represents the **points the player scores in that particular GW**\n",
    "- (Points here mean FPL points, not number of goals)\n",
    "\n",
    "*P.S. The story of this finding is definitely not linear - you can imagine how desperate it is to work with such data in real life.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6981ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 9 columns from playerstats_mapped.csv\n",
      "Output saved to: gw_olb/playerstats_mapped_filtered.csv\n",
      "Number of rows: 27657\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to extract\n",
    "columns_to_extract = [\n",
    "    'player_name',\n",
    "    'status',\n",
    "    'chance_of_playing_next_round',\n",
    "    'chance_of_playing_this_round',\n",
    "    'now_cost',\n",
    "    'event_points',\n",
    "    'ep_next',\n",
    "    'ep_this',\n",
    "    'gw'\n",
    "]\n",
    "\n",
    "# Read only the specified columns from the CSV file\n",
    "df = pd.read_csv('gw_olb/playerstats_mapped.csv', usecols=columns_to_extract)\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "df.to_csv('gw_olb/playerstats_mapped_filtered.csv', index=False)\n",
    "\n",
    "print(f\"Successfully extracted {len(columns_to_extract)} columns from playerstats_mapped.csv\")\n",
    "print(f\"Output saved to: gw_olb/playerstats_mapped_filtered.csv\")\n",
    "print(f\"Number of rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d5b4b",
   "metadata": {},
   "source": [
    "## Step 3: Check for Duplicate (gw, name) Pairs in OLB Dataset\n",
    "\n",
    "**Context**: In Premier League, it is general to see every club has only one match (fixture) in every gameweek.\n",
    "\n",
    "**Verification**: Check if `playerstats_mapped_filtered.csv` contains any duplicate (gw=i, name=j) pairs.\n",
    "\n",
    "**Expected Result**: All pairs (gw, name) should be distinct and unique in this dataset.\n",
    "\n",
    "**Note**: However, this is not the case for the dataset obtained from vaastav (as we'll see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b637ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Checking: OLB: playerstats_mapped_filtered.csv\n",
      "File: gw_olb/playerstats_mapped_filtered.csv\n",
      "Columns: gw='gw', name='player_name'\n",
      "================================================================================\n",
      "\n",
      "Total rows: 27,657\n",
      "\n",
      " PASS: No duplicate (gw, name) combinations found!\n",
      "  Total unique (gw, name) combinations: 27,657\n",
      "  Gameweeks present: 1 to 38\n",
      "  Number of unique players: 804\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates_olb(file_path, gw_col, name_col, file_description):\n",
    "    \"\"\"\n",
    "    Check for duplicate (gw, name) combinations in a CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Checking: {file_description}\")\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Columns: gw='{gw_col}', name='{name_col}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_rows = len(df)\n",
    "        print(f\"\\nTotal rows: {total_rows:,}\")\n",
    "        \n",
    "        # Check for null values\n",
    "        null_gw = df[gw_col].isnull().sum()\n",
    "        null_name = df[name_col].isnull().sum()\n",
    "        \n",
    "        if null_gw > 0:\n",
    "            print(f\"WARNING: {null_gw} null values found in '{gw_col}' column\")\n",
    "        if null_name > 0:\n",
    "            print(f\"WARNING: {null_name} null values found in '{name_col}' column\")\n",
    "        \n",
    "        # Group by (gw, name) and count occurrences\n",
    "        duplicates = df.groupby([gw_col, name_col]).size().reset_index(name='count')\n",
    "        duplicates = duplicates[duplicates['count'] > 1].sort_values('count', ascending=False)\n",
    "        \n",
    "        # Report results\n",
    "        if len(duplicates) == 0:\n",
    "            print(f\"\\n PASS: No duplicate (gw, name) combinations found!\")\n",
    "            unique_combinations = len(df.groupby([gw_col, name_col]))\n",
    "            print(f\"  Total unique (gw, name) combinations: {unique_combinations:,}\")\n",
    "            \n",
    "            # Show gameweek coverage\n",
    "            gw_coverage = sorted(df[gw_col].unique())\n",
    "            print(f\"  Gameweeks present: {min(gw_coverage)} to {max(gw_coverage)}\")\n",
    "            print(f\"  Number of unique players: {df[name_col].nunique():,}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\n FAIL: Found {len(duplicates)} duplicate (gw, name) combinations!\")\n",
    "            print(f\"\\nDuplicate combinations (showing all):\")\n",
    "            print(f\"{'GW':<6} {'Player Name':<40} {'Count':<6}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for _, row in duplicates.iterrows():\n",
    "                print(f\"{row[gw_col]:<6} {str(row[name_col])[:40]:<40} {row['count']:<6}\")\n",
    "            \n",
    "            # Show total affected rows\n",
    "            total_duplicate_rows = duplicates['count'].sum()\n",
    "            print(f\"\\nTotal rows involved in duplicates: {total_duplicate_rows:,}\")\n",
    "            return False\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found: {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Check OLB file\n",
    "olb_ok = check_duplicates_olb(\n",
    "    file_path='gw_olb/playerstats_mapped_filtered.csv',\n",
    "    gw_col='gw',\n",
    "    name_col='player_name',\n",
    "    file_description='OLB: playerstats_mapped_filtered.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422a9bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Problem from vaastav (VAA Dataset)\n",
    "\n",
    "**Datasets used**: `gw1.csv` to `gw38.csv`\n",
    "\n",
    "## Step 1: Check GW Column Consistency\n",
    "\n",
    "**Problem**: Failed to merge gw1 to gw38 initially.\n",
    "\n",
    "**Investigation**: Found that `gw22.csv` to `gw38.csv` have additional columns.\n",
    "\n",
    "**Conclusion**: Seems to be a structural change from VAA's data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6d8585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved to gw_column_check_report.txt\n",
      "\n",
      "  - gw34: 48 columns (expected 41)\n",
      "    Extra columns: ['mng_loss', 'mng_draw', 'mng_underdog_win', 'mng_goals_scored', 'mng_clean_sheets', 'mng_underdog_draw', 'mng_win']\n",
      "  - gw35: 48 columns (expected 41)\n",
      "    Extra columns: ['mng_loss', 'mng_draw', 'mng_underdog_win', 'mng_goals_scored', 'mng_clean_sheets', 'mng_underdog_draw', 'mng_win']\n",
      "  - gw36: 48 columns (expected 41)\n",
      "    Extra columns: ['mng_loss', 'mng_draw', 'mng_underdog_win', 'mng_goals_scored', 'mng_clean_sheets', 'mng_underdog_draw', 'mng_win']\n",
      "  - gw37: 48 columns (expected 41)\n",
      "    Extra columns: ['mng_loss', 'mng_draw', 'mng_underdog_win', 'mng_goals_scored', 'mng_clean_sheets', 'mng_underdog_draw', 'mng_win']\n",
      "  - gw38: 48 columns (expected 41)\n",
      "    Extra columns: ['mng_loss', 'mng_draw', 'mng_underdog_win', 'mng_goals_scored', 'mng_clean_sheets', 'mng_underdog_draw', 'mng_win']\n",
      "  - 21/38 files have correct count (41 columns)\n",
      "\n",
      "Stage 2: Column Order Check\n",
      "(Only checking 21 files with correct column count)\n",
      " All 21 files with correct count have matching column order\n",
      "\n",
      "Overall: 21/38 files match gw1.csv perfectly\n"
     ]
    }
   ],
   "source": [
    "def check_column_consistency():\n",
    "    # Read reference file (gw1.csv)\n",
    "    reference_file = 'gw_vaa/gw1.csv'\n",
    "    with open(reference_file, 'r', encoding='utf-8') as f:\n",
    "        reference_columns = next(csv.reader(f))\n",
    "    \n",
    "    reference_count = len(reference_columns)\n",
    "    \n",
    "    # Prepare output\n",
    "    output_lines = []\n",
    "    output_lines.append(\"=== Column Check Report ===\")\n",
    "    output_lines.append(f\"Reference: {reference_file}\\n\")\n",
    "    \n",
    "    # Check all files and collect information\n",
    "    all_files_info = []\n",
    "    count_mismatches = []\n",
    "    order_mismatches = []\n",
    "    \n",
    "    for i in range(1, 39):  # gw1 to gw38\n",
    "        filename = f'gw_vaa/gw{i}.csv'\n",
    "        \n",
    "        # Read header\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            columns = next(csv.reader(f))\n",
    "        \n",
    "        column_count = len(columns)\n",
    "        \n",
    "        # Store file info\n",
    "        all_files_info.append({\n",
    "            'name': f'gw{i}',\n",
    "            'count': column_count,\n",
    "            'columns': columns\n",
    "        })\n",
    "        \n",
    "        # Output file info\n",
    "        columns_str = str(columns)\n",
    "        output_lines.append(f\"{all_files_info[-1]['name']} : number of column = {column_count}, column: {columns_str}\")\n",
    "        \n",
    "        # Check for differences (skip gw1 as it's the reference)\n",
    "        if i > 1:\n",
    "            # Check column count\n",
    "            if column_count != reference_count:\n",
    "                missing = set(reference_columns) - set(columns)\n",
    "                extra = set(columns) - set(reference_columns)\n",
    "                count_mismatches.append({\n",
    "                    'file': f'gw{i}',\n",
    "                    'count': column_count,\n",
    "                    'missing': list(missing),\n",
    "                    'extra': list(extra)\n",
    "                })\n",
    "            # Check column order (only if same count)\n",
    "            elif columns != reference_columns:\n",
    "                # Find position differences\n",
    "                differences = []\n",
    "                for idx, (ref_col, file_col) in enumerate(zip(reference_columns, columns)):\n",
    "                    if ref_col != file_col:\n",
    "                        differences.append(f\"Position {idx}: '{ref_col}' in gw1, '{file_col}' in gw{i}\")\n",
    "                \n",
    "                order_mismatches.append({\n",
    "                    'file': f'gw{i}',\n",
    "                    'differences': differences\n",
    "                })\n",
    "    \n",
    "    # Generate diagnosis\n",
    "    output_lines.append(\"\\n=== Diagnosis Result ===\\n\")\n",
    "    \n",
    "    # Stage 1: Column Count Check\n",
    "    output_lines.append(\"Stage 1: Column Count Check\")\n",
    "    if not count_mismatches:\n",
    "        output_lines.append(f\" All files have {reference_count} columns (same as gw1.csv)\\n\")\n",
    "    else:\n",
    "        output_lines.append(f\" Column count differences found:\")\n",
    "        for mismatch in count_mismatches:\n",
    "            output_lines.append(f\"  - {mismatch['file']}: {mismatch['count']} columns (expected {reference_count})\")\n",
    "            if mismatch['missing']:\n",
    "                output_lines.append(f\"    Missing columns: {mismatch['missing']}\")\n",
    "            if mismatch['extra']:\n",
    "                output_lines.append(f\"    Extra columns: {mismatch['extra']}\")\n",
    "        files_with_correct_count = 38 - len(count_mismatches)\n",
    "        output_lines.append(f\"  - {files_with_correct_count}/38 files have correct count ({reference_count} columns)\\n\")\n",
    "    \n",
    "    # Stage 2: Column Order Check\n",
    "    output_lines.append(\"Stage 2: Column Order Check\")\n",
    "    if count_mismatches:\n",
    "        files_to_check = 38 - len(count_mismatches)\n",
    "        output_lines.append(f\"(Only checking {files_to_check} files with correct column count)\")\n",
    "    \n",
    "    if not order_mismatches:\n",
    "        if count_mismatches:\n",
    "            files_matching = 38 - len(count_mismatches)\n",
    "            output_lines.append(f\" All {files_matching} files with correct count have matching column order\\n\")\n",
    "        else:\n",
    "            output_lines.append(\" All files have matching column order\\n\")\n",
    "    else:\n",
    "        output_lines.append(\" Found column order differences:\")\n",
    "        for mismatch in order_mismatches:\n",
    "            output_lines.append(f\"  - {mismatch['file']}:\")\n",
    "            for diff in mismatch['differences']:\n",
    "                output_lines.append(f\"    {diff}\")\n",
    "        output_lines.append(\"\")\n",
    "    \n",
    "    # Overall summary\n",
    "    total_matching = 38 - len(count_mismatches) - len(order_mismatches)\n",
    "    output_lines.append(f\"Overall: {total_matching}/38 files match gw1.csv perfectly\")\n",
    "    \n",
    "    # Write to file\n",
    "    output_file = 'gw_column_check_report.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "    \n",
    "    print(f\"Report saved to {output_file}\")\n",
    "    print(\"\\n\" + '\\n'.join(output_lines[-15:]))  # Print summary to console\n",
    "\n",
    "check_column_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f733b3",
   "metadata": {},
   "source": [
    "## Step 2: Merge GW Files\n",
    "\n",
    "**Solution**: Merge gw1 to gw38 **without** the excessive columns found from gw22 to gw38.\n",
    "\n",
    "**Process**:\n",
    "1. Validate column consistency after removing extra columns\n",
    "2. Merge all files with proper column alignment\n",
    "3. Add 'gw' column to identify gameweek\n",
    "\n",
    "**Extra columns to remove** (from gw22-gw38): `mng_clean_sheets`, `mng_draw`, `mng_goals_scored`, `mng_loss`, `mng_underdog_draw`, `mng_underdog_win`, `mng_win`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430db41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference (gw1): 41 columns\n",
      "Expected columns: ['name', 'position', 'team', 'xP', 'assists', 'bonus', 'bps', 'clean_sheets', 'creativity', 'element', 'expected_assists', 'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'fixture', 'goals_conceded', 'goals_scored', 'ict_index', 'influence', 'kickoff_time', 'minutes', 'modified', 'opponent_team', 'own_goals', 'penalties_missed', 'penalties_saved', 'red_cards', 'round', 'saves', 'selected', 'starts', 'team_a_score', 'team_h_score', 'threat', 'total_points', 'transfers_balance', 'transfers_in', 'transfers_out', 'value', 'was_home', 'yellow_cards']\n",
      "\n",
      "============================================================\n",
      "STEP 1: Validating column order for gw22-gw38\n",
      "============================================================\n",
      " gw22: Column order matches after removing extra columns\n",
      " gw23: Column order matches after removing extra columns\n",
      " gw24: Column order matches after removing extra columns\n",
      " gw25: Column order matches after removing extra columns\n",
      " gw26: Column order matches after removing extra columns\n",
      " gw27: Column order matches after removing extra columns\n",
      " gw28: Column order matches after removing extra columns\n",
      " gw29: Column order matches after removing extra columns\n",
      " gw30: Column order matches after removing extra columns\n",
      " gw31: Column order matches after removing extra columns\n",
      " gw32: Column order matches after removing extra columns\n",
      " gw33: Column order matches after removing extra columns\n",
      " gw34: Column order matches after removing extra columns\n",
      " gw35: Column order matches after removing extra columns\n",
      " gw36: Column order matches after removing extra columns\n",
      " gw37: Column order matches after removing extra columns\n",
      " gw38: Column order matches after removing extra columns\n",
      "\n",
      "============================================================\n",
      " VALIDATION PASSED: All files have matching column order\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 2: Merging all files into gw_merged.csv\n",
      "============================================================\n",
      " Header written (42 columns, including 'gw')\n",
      " gw1: 616 rows added\n",
      " gw2: 627 rows added\n",
      " gw3: 648 rows added\n",
      " gw4: 659 rows added\n",
      " gw5: 661 rows added\n",
      " gw6: 664 rows added\n",
      " gw7: 666 rows added\n",
      " gw8: 667 rows added\n",
      " gw9: 670 rows added\n",
      " gw10: 674 rows added\n",
      " gw11: 678 rows added\n",
      " gw12: 684 rows added\n",
      " gw13: 690 rows added\n",
      " gw14: 693 rows added\n",
      " gw15: 630 rows added\n",
      " gw16: 701 rows added\n",
      " gw17: 701 rows added\n",
      " gw18: 705 rows added\n",
      " gw19: 709 rows added\n",
      " gw20: 711 rows added\n",
      " gw21: 724 rows added\n",
      " gw22: 729 rows added\n",
      " gw23: 756 rows added\n",
      " gw24: 831 rows added\n",
      " gw25: 858 rows added\n",
      " gw26: 783 rows added\n",
      " gw27: 784 rows added\n",
      " gw28: 789 rows added\n",
      " gw29: 639 rows added\n",
      " gw30: 791 rows added\n",
      " gw31: 792 rows added\n",
      " gw32: 869 rows added\n",
      " gw33: 954 rows added\n",
      " gw34: 645 rows added\n",
      " gw35: 801 rows added\n",
      " gw36: 801 rows added\n",
      " gw37: 801 rows added\n",
      " gw38: 804 rows added\n",
      "\n",
      "============================================================\n",
      "MERGE COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Output file: gw_vaa/gw_merged.csv\n",
      "Total rows: 27605 (excluding header)\n",
      "Total columns: 42 (including 'gw' column)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_and_merge_gw_files():\n",
    "    extra_columns = [\n",
    "        'mng_clean_sheets', 'mng_draw', 'mng_goals_scored', 'mng_loss',\n",
    "        'mng_underdog_draw', 'mng_underdog_win', 'mng_win'\n",
    "    ]\n",
    "    \n",
    "    reference_file = 'gw_vaa/gw1.csv'\n",
    "    with open(reference_file, 'r', encoding='utf-8') as f:\n",
    "        reference_columns = next(csv.reader(f))\n",
    "    \n",
    "    print(f\"Reference (gw1): {len(reference_columns)} columns\")\n",
    "    print(f\"Expected columns: {reference_columns}\\n\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: Validating column order for gw22-gw38\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    validation_passed = True\n",
    "    column_indices_to_keep = {}\n",
    "    \n",
    "    for i in range(22, 39):\n",
    "        filename = f'gw_vaa/gw{i}.csv'\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            columns = next(csv.reader(f))\n",
    "        \n",
    "        filtered_columns = [col for col in columns if col not in extra_columns]\n",
    "        indices_to_keep = [idx for idx, col in enumerate(columns) if col not in extra_columns]\n",
    "        column_indices_to_keep[i] = indices_to_keep\n",
    "        \n",
    "        if filtered_columns == reference_columns:\n",
    "            print(f\" gw{i}: Column order matches after removing extra columns\")\n",
    "        else:\n",
    "            print(f\" gw{i}: Column order DOES NOT match!\")\n",
    "            validation_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if validation_passed:\n",
    "        print(\" VALIDATION PASSED: All files have matching column order\")\n",
    "    else:\n",
    "        print(\" VALIDATION FAILED: Column order mismatch detected\")\n",
    "        print(\"Merge operation ABORTED.\")\n",
    "        return False\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Merging all files into gw_merged.csv\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    output_file = 'gw_vaa/gw_merged.csv'\n",
    "    total_rows = 0\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        header_with_gw = reference_columns + ['gw']\n",
    "        writer.writerow(header_with_gw)\n",
    "        print(f\" Header written ({len(header_with_gw)} columns, including 'gw')\")\n",
    "        \n",
    "        for i in range(1, 39):\n",
    "            filename = f'gw_vaa/gw{i}.csv'\n",
    "            file_rows = 0\n",
    "            \n",
    "            with open(filename, 'r', encoding='utf-8') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                next(reader)\n",
    "                \n",
    "                if i <= 21:\n",
    "                    for row in reader:\n",
    "                        row_with_gw = row + [str(i)]\n",
    "                        writer.writerow(row_with_gw)\n",
    "                        file_rows += 1\n",
    "                        total_rows += 1\n",
    "                else:\n",
    "                    indices = column_indices_to_keep[i]\n",
    "                    for row in reader:\n",
    "                        filtered_row = [row[idx] for idx in indices] + [str(i)]\n",
    "                        writer.writerow(filtered_row)\n",
    "                        file_rows += 1\n",
    "                        total_rows += 1\n",
    "            \n",
    "            print(f\" gw{i}: {file_rows} rows added\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MERGE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Total rows: {total_rows} (excluding header)\")\n",
    "    print(f\"Total columns: {len(header_with_gw)} (including 'gw' column)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "validate_and_merge_gw_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771193b",
   "metadata": {},
   "source": [
    "## Step 3: Filter GW Merged Columns\n",
    "\n",
    "**Problem**: After merging, there are too many variables unnecessary for the study.\n",
    "\n",
    "**Solution**: Keep only the following variables: `name`, `position`, `team`, `xP`, `minutes`, `starts`, `total_points`, `value`, `gw`\n",
    "\n",
    "**Row Count After This Step**:\n",
    "- gw_merged_filtered.csv: **27,605 rows**\n",
    "- playerstats_mapped_filtered.csv: **27,658 rows**\n",
    "\n",
    "A serious cross-referencing has to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ced4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 9 columns from gw_merged.csv\n",
      "Output saved to: gw_vaa/gw_merged_filtered.csv\n",
      "Number of rows: 27605\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to extract\n",
    "columns_to_extract = [\n",
    "    'name',\n",
    "    'position',\n",
    "    'team',\n",
    "    'xP',\n",
    "    'minutes',\n",
    "    'starts',\n",
    "    'total_points',\n",
    "    'value',\n",
    "    'gw'\n",
    "]\n",
    "\n",
    "# Read only the specified columns from the CSV file\n",
    "df = pd.read_csv('gw_vaa/gw_merged.csv', usecols=columns_to_extract)\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "df.to_csv('gw_vaa/gw_merged_filtered.csv', index=False)\n",
    "\n",
    "print(f\"Successfully extracted {len(columns_to_extract)} columns from gw_merged.csv\")\n",
    "print(f\"Output saved to: gw_vaa/gw_merged_filtered.csv\")\n",
    "print(f\"Number of rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403088c7",
   "metadata": {},
   "source": [
    "## Step 4: Check for Duplicate (gw, name) Pairs in VAA Dataset\n",
    "\n",
    "**Investigation**: Check if there exist repeated pairs of (gw=i, name=j) in the VAA dataset.\n",
    "\n",
    "**Finding**: Duplicates exist, and most of them occurred in GW 24, 25, 32, and 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d549d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Checking: VAA gw_merged_filtered.csv\n",
      "File: gw_vaa/gw_merged_filtered.csv\n",
      "================================================================================\n",
      "\n",
      "Total rows: 27,605\n",
      "\n",
      " Found 374 duplicate (gw, name) combinations!\n",
      "\n",
      "Duplicate distribution by GW:\n",
      "  GW 24: 69 duplicate pairs\n",
      "  GW 25: 79 duplicate pairs\n",
      "  GW 32: 71 duplicate pairs\n",
      "  GW 33: 155 duplicate pairs\n",
      "\n",
      "Sample of duplicate pairs:\n",
      "       gw                       name  count\n",
      "23373  33                 Zach Marsh      2\n",
      "15667  24         Abdoulaye Doucouré      2\n",
      "15687  24        Alexis Mac Allister      2\n",
      "15694  24      Alisson Ramses Becker      2\n",
      "15699  24                Amara Nallo      2\n",
      "15705  24           Andrew Robertson      2\n",
      "23270  33               Scott Carson      2\n",
      "23263  33        Samuel Iling-Junior      2\n",
      "23256  33  Salah-Eddine Oulad M'hand      2\n",
      "23255  33      Rúben Gato Alves Dias      2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_60580\\72219451.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"  GW {row[0]}: {row['pairs']} duplicate pairs\")\n"
     ]
    }
   ],
   "source": [
    "# Check VAA file for duplicates\n",
    "vaa_file_path = 'gw_vaa/gw_merged_filtered.csv'\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Checking: VAA gw_merged_filtered.csv\")\n",
    "print(f\"File: {vaa_file_path}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "df_vaa = pd.read_csv(vaa_file_path)\n",
    "total_rows = len(df_vaa)\n",
    "print(f\"\\nTotal rows: {total_rows:,}\")\n",
    "\n",
    "# Group by (gw, name) and count occurrences\n",
    "duplicates = df_vaa.groupby(['gw', 'name']).size().reset_index(name='count')\n",
    "duplicates = duplicates[duplicates['count'] > 1].sort_values('count', ascending=False)\n",
    "\n",
    "if len(duplicates) == 0:\n",
    "    print(f\"\\n PASS: No duplicate (gw, name) combinations found!\")\n",
    "else:\n",
    "    print(f\"\\n Found {len(duplicates)} duplicate (gw, name) combinations!\")\n",
    "    \n",
    "    # Distribution by gameweek\n",
    "    dup_by_gw = duplicates.groupby(df_vaa.loc[df_vaa.set_index(['gw', 'name']).index.isin(\n",
    "        pd.MultiIndex.from_frame(duplicates[['gw', 'name']])\n",
    "    ), 'gw']).size().reset_index(name='dup_count')\n",
    "    \n",
    "    print(f\"\\nDuplicate distribution by GW:\")\n",
    "    for _, row in duplicates.groupby(duplicates['gw'].map(lambda x: x)).size().reset_index(name='pairs').iterrows():\n",
    "        print(f\"  GW {row[0]}: {row['pairs']} duplicate pairs\")\n",
    "    \n",
    "    print(f\"\\nSample of duplicate pairs:\")\n",
    "    print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cbbb2",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Duplicate Rows\n",
    "\n",
    "**Investigation**: Further analysis shows that:\n",
    "1. The count of occurrence of all repeated pairs is **2** (each duplicated pair has exactly 2 rows)\n",
    "2. Those duplicated pairs have **different data**\n",
    "\n",
    "**Explanation**: By inspection of the unfiltered merged dataset, in GW 24, 25, 32, 33, some clubs had **two fixtures (matches)** rather than one because of rescheduling due to weather or other reasons. This is confirmed by different kickoff times and enemy team codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3fac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DUPLICATE ROW ANALYSIS\n",
      "File: gw_vaa/gw_merged_filtered.csv\n",
      "Checking if duplicate (gw, name) pairs have identical or different rows\n",
      "================================================================================\n",
      "\n",
      "Total rows in file: 27,605\n",
      "Total columns: 9\n",
      "\n",
      "Found 374 duplicate (gw, name) pairs\n",
      "Total rows involved: 748\n",
      "\n",
      "Analyzing duplicate pairs...\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS RESULTS\n",
      "================================================================================\n",
      "\n",
      " Identical duplicates: 202 pairs\n",
      "  (These are exact copies - can be safely deduplicated)\n",
      "\n",
      " Different data duplicates: 172 pairs\n",
      "  (These have different values - need aggregation)\n",
      "\n",
      "================================================================================\n",
      "DIFFERENT DATA DUPLICATES - DETAILED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "--- Duplicate #1: GW 24, Abdoulaye Doucouré ---\n",
      "Number of occurrences: 2\n",
      "\n",
      "All 2 rows for this (gw, name) pair:\n",
      "              name position    team  xP  minutes  starts  total_points  value  gw\n",
      "Abdoulaye Doucouré      MID Everton 4.2       90       1             8     51  24\n",
      "Abdoulaye Doucouré      MID Everton 4.2       90       1            -1     51  24\n",
      "\n",
      "Columns with different values:\n",
      "  - total_points: [ 8 -1]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Duplicate #2: GW 24, Alexis Mac Allister ---\n",
      "Number of occurrences: 2\n",
      "\n",
      "All 2 rows for this (gw, name) pair:\n",
      "               name position      team  xP  minutes  starts  total_points  value  gw\n",
      "Alexis Mac Allister      MID Liverpool 8.7       60       1             2     62  24\n",
      "Alexis Mac Allister      MID Liverpool 8.7       90       1             8     62  24\n",
      "\n",
      "Columns with different values:\n",
      "  - minutes: [60 90]\n",
      "  - total_points: [2 8]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Duplicate #3: GW 24, Alisson Ramses Becker ---\n",
      "Number of occurrences: 2\n",
      "\n",
      "All 2 rows for this (gw, name) pair:\n",
      "                 name position      team   xP  minutes  starts  total_points  value  gw\n",
      "Alisson Ramses Becker       GK Liverpool 10.7       90       1             9     55  24\n",
      "Alisson Ramses Becker       GK Liverpool 10.7       90       1             1     55  24\n",
      "\n",
      "Columns with different values:\n",
      "  - total_points: [9 1]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Duplicate #4: GW 24, Andrew Robertson ---\n",
      "Number of occurrences: 2\n",
      "\n",
      "All 2 rows for this (gw, name) pair:\n",
      "            name position      team  xP  minutes  starts  total_points  value  gw\n",
      "Andrew Robertson      DEF Liverpool 6.7       90       1             7     59  24\n",
      "Andrew Robertson      DEF Liverpool 6.7       68       1             1     59  24\n",
      "\n",
      "Columns with different values:\n",
      "  - minutes: [90 68]\n",
      "  - total_points: [7 1]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Duplicate #5: GW 24, Arne Slot ---\n",
      "Number of occurrences: 2\n",
      "\n",
      "All 2 rows for this (gw, name) pair:\n",
      "     name position      team   xP  minutes  starts  total_points  value  gw\n",
      "Arne Slot       AM Liverpool 18.1        0       0            10     15  24\n",
      "Arne Slot       AM Liverpool 18.1        0       0             5     15  24\n",
      "\n",
      "Columns with different values:\n",
      "  - total_points: [10  5]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "... and 167 more pairs with different data\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_duplicate_rows(file_path, gw_col, name_col):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DUPLICATE ROW ANALYSIS\")\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Checking if duplicate (gw, name) pairs have identical or different rows\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"\\nTotal rows in file: {len(df):,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Find duplicate (gw, name) pairs\n",
    "    duplicate_pairs = df.groupby([gw_col, name_col]).size().reset_index(name='count')\n",
    "    duplicate_pairs = duplicate_pairs[duplicate_pairs['count'] > 1]\n",
    "    \n",
    "    if len(duplicate_pairs) == 0:\n",
    "        print(\"\\n No duplicate (gw, name) pairs found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(duplicate_pairs)} duplicate (gw, name) pairs\")\n",
    "    print(f\"Total rows involved: {duplicate_pairs['count'].sum()}\")\n",
    "    \n",
    "    # Analyze each duplicate pair\n",
    "    identical_duplicates = []\n",
    "    different_duplicates = []\n",
    "    \n",
    "    print(f\"\\nAnalyzing duplicate pairs...\")\n",
    "    \n",
    "    for idx, row in duplicate_pairs.iterrows():\n",
    "        gw_val = row[gw_col]\n",
    "        name_val = row[name_col]\n",
    "        count = row['count']\n",
    "        \n",
    "        # Get all rows for this (gw, name) pair\n",
    "        mask = (df[gw_col] == gw_val) & (df[name_col] == name_val)\n",
    "        duplicate_rows = df[mask]\n",
    "        \n",
    "        # Check if all rows are identical\n",
    "        first_row = duplicate_rows.iloc[0]\n",
    "        all_identical = True\n",
    "        \n",
    "        for i in range(1, len(duplicate_rows)):\n",
    "            if not duplicate_rows.iloc[i].equals(first_row):\n",
    "                all_identical = False\n",
    "                break\n",
    "        \n",
    "        if all_identical:\n",
    "            identical_duplicates.append({\n",
    "                'gw': gw_val,\n",
    "                'name': name_val,\n",
    "                'count': count\n",
    "            })\n",
    "        else:\n",
    "            different_duplicates.append({\n",
    "                'gw': gw_val,\n",
    "                'name': name_val,\n",
    "                'count': count,\n",
    "                'rows': duplicate_rows\n",
    "            })\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ANALYSIS RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\n Identical duplicates: {len(identical_duplicates)} pairs\")\n",
    "    print(f\"  (These are exact copies - can be safely deduplicated)\")\n",
    "    \n",
    "    print(f\"\\n Different data duplicates: {len(different_duplicates)} pairs\")\n",
    "    if len(different_duplicates) > 0:\n",
    "        print(f\"  (These have different values - need aggregation)\")\n",
    "    \n",
    "    # Show detailed analysis of different duplicates\n",
    "    if len(different_duplicates) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DIFFERENT DATA DUPLICATES - DETAILED ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, dup in enumerate(different_duplicates[:5]):  # Show first 5\n",
    "            print(f\"\\n--- Duplicate #{i+1}: GW {dup['gw']}, {dup['name']} ---\")\n",
    "            print(f\"Number of occurrences: {dup['count']}\")\n",
    "            \n",
    "            # Show all rows for this duplicate\n",
    "            rows = dup['rows']\n",
    "            print(f\"\\nAll {len(rows)} rows for this (gw, name) pair:\")\n",
    "            print(rows.to_string(index=False))\n",
    "            \n",
    "            # Find which columns differ\n",
    "            print(f\"\\nColumns with different values:\")\n",
    "            differing_cols = []\n",
    "            for col in rows.columns:\n",
    "                if rows[col].nunique() > 1:\n",
    "                    differing_cols.append(col)\n",
    "                    unique_vals = rows[col].unique()\n",
    "                    print(f\"  - {col}: {unique_vals}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        if len(different_duplicates) > 5:\n",
    "            print(f\"\\n... and {len(different_duplicates) - 5} more pairs with different data\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "analyze_duplicate_rows(\n",
    "    file_path='gw_vaa/gw_merged_filtered.csv',\n",
    "    gw_col='gw',\n",
    "    name_col='name'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10cc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Problem from Cross-Referencing\n",
    "\n",
    "Before combining the two data sources, we need to handle the duplicates and verify data consistency.\n",
    "\n",
    "## Step 1: Verify Duplicate Points\n",
    "\n",
    "**Hypothesis**: In VAA's dataset, for those players who have duplicated pair data rows (gw=i, name=j), the **sum of 'total_points' from two matches in that gw** should equal the **'event_points' at (gw=i, name=j) in OLB's dataset**.\n",
    "\n",
    "This will verify data consistency between the two sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a4cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "VAA dataset: 27605 rows\n",
      "OLB dataset: 27657 rows\n",
      "\n",
      "Identifying duplicated (gw, name) pairs in VAA dataset...\n",
      "Found 748 rows with duplicated (gw, name) pairs\n",
      "\n",
      "Number of unique duplicated (gw, name) pairs: 374\n",
      "\n",
      "Distribution of duplicate counts:\n",
      "2    374\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Summing total_points for each duplicated pair...\n",
      "Created 374 summed records\n",
      "\n",
      "Merging with OLB dataset...\n",
      "Merged dataset has 374 rows\n",
      "\n",
      "Comparing summed total_points with event_points...\n",
      "======================================================================\n",
      "VERIFICATION RESULTS\n",
      "======================================================================\n",
      "Total duplicated (gw, name) pairs analyzed: 374\n",
      "Exact matches (VAA sum == OLB event_points): 374\n",
      "Mismatches (VAA sum != OLB event_points): 0\n",
      "\n",
      "HYPOTHESIS VERIFICATION: PASSED\n",
      "All duplicated pairs' summed total_points match event_points exactly!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "vaa_df = pd.read_csv('gw_vaa/gw_merged_filtered.csv')\n",
    "olb_df = pd.read_csv('gw_olb/playerstats_mapped_filtered.csv')\n",
    "\n",
    "print(f\"VAA dataset: {len(vaa_df)} rows\")\n",
    "print(f\"OLB dataset: {len(olb_df)} rows\")\n",
    "print()\n",
    "\n",
    "# Find duplicated (gw, name) pairs in VAA dataset\n",
    "print(\"Identifying duplicated (gw, name) pairs in VAA dataset...\")\n",
    "duplicated_mask = vaa_df.duplicated(subset=['gw', 'name'], keep=False)\n",
    "duplicated_df = vaa_df[duplicated_mask].copy()\n",
    "\n",
    "print(f\"Found {len(duplicated_df)} rows with duplicated (gw, name) pairs\")\n",
    "print()\n",
    "\n",
    "# Count unique duplicated pairs\n",
    "unique_pairs = duplicated_df.groupby(['gw', 'name']).size()\n",
    "print(f\"Number of unique duplicated (gw, name) pairs: {len(unique_pairs)}\")\n",
    "print()\n",
    "\n",
    "# Check that all duplicated pairs appear exactly twice\n",
    "pair_counts = unique_pairs.value_counts()\n",
    "print(\"Distribution of duplicate counts:\")\n",
    "print(pair_counts)\n",
    "print()\n",
    "\n",
    "if len(pair_counts) > 1 or (len(pair_counts) == 1 and pair_counts.index[0] != 2):\n",
    "    print(\"WARNING: Not all duplicated pairs appear exactly twice!\")\n",
    "    print(\"Pairs with counts != 2:\")\n",
    "    non_two_pairs = unique_pairs[unique_pairs != 2]\n",
    "    print(non_two_pairs)\n",
    "    print()\n",
    "\n",
    "# Sum total_points for each duplicated (gw, name) pair\n",
    "print(\"Summing total_points for each duplicated pair...\")\n",
    "vaa_summed = duplicated_df.groupby(['gw', 'name'])['total_points'].sum().reset_index()\n",
    "vaa_summed.columns = ['gw', 'name', 'vaa_sum_total_points']\n",
    "\n",
    "print(f\"Created {len(vaa_summed)} summed records\")\n",
    "print()\n",
    "\n",
    "# Merge with OLB dataset\n",
    "print(\"Merging with OLB dataset...\")\n",
    "merged = vaa_summed.merge(\n",
    "    olb_df[['gw', 'player_name', 'event_points']],\n",
    "    left_on=['gw', 'name'],\n",
    "    right_on=['gw', 'player_name'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset has {len(merged)} rows\")\n",
    "print()\n",
    "\n",
    "# Check for unmatched records\n",
    "unmatched = merged[merged['event_points'].isna()]\n",
    "if len(unmatched) > 0:\n",
    "    print(f\"WARNING: {len(unmatched)} duplicated pairs from VAA not found in OLB dataset:\")\n",
    "    print(unmatched[['gw', 'name', 'vaa_sum_total_points']])\n",
    "    print()\n",
    "\n",
    "# Remove unmatched records for comparison\n",
    "merged_matched = merged[merged['event_points'].notna()].copy()\n",
    "\n",
    "# Compare values (exact integer comparison)\n",
    "print(\"Comparing summed total_points with event_points...\")\n",
    "merged_matched['match'] = merged_matched['vaa_sum_total_points'] == merged_matched['event_points']\n",
    "\n",
    "# Count matches and mismatches\n",
    "num_matches = merged_matched['match'].sum()\n",
    "num_mismatches = (~merged_matched['match']).sum()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total duplicated (gw, name) pairs analyzed: {len(merged_matched)}\")\n",
    "print(f\"Exact matches (VAA sum == OLB event_points): {num_matches}\")\n",
    "print(f\"Mismatches (VAA sum != OLB event_points): {num_mismatches}\")\n",
    "print()\n",
    "\n",
    "if num_mismatches > 0:\n",
    "    print(\"HYPOTHESIS VERIFICATION: FAILED\")\n",
    "    print(f\"Found {num_mismatches} mismatches\")\n",
    "    print()\n",
    "    print(\"Details of mismatches:\")\n",
    "    print(\"-\" * 70)\n",
    "    mismatches = merged_matched[~merged_matched['match']][['gw', 'name', 'vaa_sum_total_points', 'event_points']]\n",
    "    mismatches['difference'] = mismatches['vaa_sum_total_points'] - mismatches['event_points']\n",
    "    print(mismatches.to_string(index=False))\n",
    "else:\n",
    "    print(\"HYPOTHESIS VERIFICATION: PASSED\")\n",
    "    print(\"All duplicated pairs' summed total_points match event_points exactly!\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4b81b",
   "metadata": {},
   "source": [
    "## Step 2: Merge Duplicate GWs\n",
    "\n",
    "**Solution**: For GW 24, 25, 32, and 33, for those players having two matches, the following data manipulation will be done:\n",
    "\n",
    "**Aggregation Rules** (assuming name, position, team are unchanged):\n",
    "- **Sum**: xP, total_points\n",
    "- **Max**: starts\n",
    "- **Average & round**: value (to the closest 0.1 Million), minutes (to nearest integer)\n",
    "\n",
    "Output as `gw_merged_filtered_nodup.csv`\n",
    "\n",
    "**After this step**:\n",
    "- VAA dataset will have **27,231 rows**\n",
    "- OLB dataset still has **27,658 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "353f41a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gw_merged_filtered.csv...\n",
      "Total rows: 27605\n",
      "Columns: name, position, team, xP, minutes, starts, total_points, value, gw\n",
      "\n",
      "Identifying duplicates in GWs [24, 25, 32, 33]...\n",
      "Found 748 duplicate rows in target GWs\n",
      "Number of unique duplicate pairs: 374\n",
      "\n",
      "Aggregating duplicate rows...\n",
      "Aggregated to 374 rows\n",
      "\n",
      "Collecting non-duplicate rows...\n",
      "Non-duplicate rows from target GWs: 2764\n",
      "Rows from other GWs: 24093\n",
      "\n",
      "Combining datasets...\n",
      "Final dataset rows: 27231\n",
      "\n",
      "Saving to gw_vaa/gw_merged_filtered_nodup.csv...\n",
      "Done!\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Input rows:  27,605\n",
      "Output rows: 27,231\n",
      "Rows removed (duplicates merged): 374\n",
      "\n",
      "Sample of aggregated duplicate rows:\n",
      "                 name position      team   xP  minutes  starts  total_points  value  gw\n",
      "   Abdoulaye Doucouré      MID   Everton  8.4       90       1             7     51  24\n",
      "  Alexis Mac Allister      MID Liverpool 17.4       75       1            10     62  24\n",
      "Alisson Ramses Becker       GK Liverpool 21.4       90       1            10     55  24\n",
      "          Amara Nallo      DEF Liverpool  3.0        0       0             0     40  24\n",
      "     Andrew Robertson      DEF Liverpool 13.4       79       1             8     59  24\n",
      "        Armando Broja      FWD   Everton  0.0        0       0             0     54  24\n",
      "            Arne Slot       AM Liverpool 36.2        0       0            15     15  24\n",
      "         Ashley Young      DEF   Everton  2.0        9       0             2     46  24\n",
      "        Asmir Begovic       GK   Everton -2.0        0       0             0     40  24\n",
      "             Ben Doak      MID Liverpool  0.0        0       0             0     45  24\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading gw_merged_filtered.csv...\")\n",
    "df = pd.read_csv('gw_vaa/gw_merged_filtered.csv')\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns: {', '.join(df.columns)}\")\n",
    "print()\n",
    "\n",
    "# Define target gameweeks\n",
    "target_gws = [24, 25, 32, 33]\n",
    "\n",
    "# Identify duplicates in target gameweeks\n",
    "print(f\"Identifying duplicates in GWs {target_gws}...\")\n",
    "target_gw_df = df[df['gw'].isin(target_gws)].copy()\n",
    "duplicated_mask = target_gw_df.duplicated(subset=['gw', 'name'], keep=False)\n",
    "duplicates_df = target_gw_df[duplicated_mask].copy()\n",
    "\n",
    "print(f\"Found {len(duplicates_df)} duplicate rows in target GWs\")\n",
    "print(f\"Number of unique duplicate pairs: {len(duplicates_df.groupby(['gw', 'name']))}\")\n",
    "print()\n",
    "\n",
    "# Aggregate duplicates\n",
    "if len(duplicates_df) > 0:\n",
    "    print(\"Aggregating duplicate rows...\")\n",
    "    \n",
    "    # Define aggregation rules\n",
    "    agg_dict = {\n",
    "        'name': 'first',\n",
    "        'position': 'first',\n",
    "        'team': 'first',\n",
    "        'xP': 'sum',\n",
    "        'minutes': lambda x: round(x.mean()),  # Average and round to nearest integer\n",
    "        'starts': 'max',\n",
    "        'total_points': 'sum',\n",
    "        'value': lambda x: round(x.mean()),  # Average and round to nearest integer\n",
    "        'gw': 'first'\n",
    "    }\n",
    "    \n",
    "    aggregated_df = duplicates_df.groupby(['gw', 'name'], as_index=False).agg(agg_dict)\n",
    "    \n",
    "    print(f\"Aggregated to {len(aggregated_df)} rows\")\n",
    "    print()\n",
    "else:\n",
    "    aggregated_df = pd.DataFrame(columns=df.columns)\n",
    "    print(\"No duplicates found to aggregate\")\n",
    "    print()\n",
    "\n",
    "# Get non-duplicate rows\n",
    "print(\"Collecting non-duplicate rows...\")\n",
    "\n",
    "# Non-duplicates from target GWs\n",
    "non_dup_target_gws = target_gw_df[~duplicated_mask].copy()\n",
    "\n",
    "# All rows from other GWs\n",
    "other_gws_df = df[~df['gw'].isin(target_gws)].copy()\n",
    "\n",
    "print(f\"Non-duplicate rows from target GWs: {len(non_dup_target_gws)}\")\n",
    "print(f\"Rows from other GWs: {len(other_gws_df)}\")\n",
    "print()\n",
    "\n",
    "# Combine all parts\n",
    "print(\"Combining datasets...\")\n",
    "result_df = pd.concat([aggregated_df, non_dup_target_gws, other_gws_df], ignore_index=True)\n",
    "\n",
    "# Sort by gw and name for consistency\n",
    "result_df = result_df.sort_values(['gw', 'name']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final dataset rows: {len(result_df)}\")\n",
    "print()\n",
    "\n",
    "# Verify column order\n",
    "expected_columns = ['name', 'position', 'team', 'xP', 'minutes', 'starts', 'total_points', 'value', 'gw']\n",
    "result_df = result_df[expected_columns]\n",
    "\n",
    "# Save the result\n",
    "output_path = 'gw_vaa/gw_merged_filtered_nodup.csv'\n",
    "print(f\"Saving to {output_path}...\")\n",
    "result_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Done!\")\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Input rows:  {len(df):,}\")\n",
    "print(f\"Output rows: {len(result_df):,}\")\n",
    "print(f\"Rows removed (duplicates merged): {len(df) - len(result_df):,}\")\n",
    "print()\n",
    "\n",
    "# Show sample of aggregated rows if any\n",
    "if len(aggregated_df) > 0:\n",
    "    print(\"Sample of aggregated duplicate rows:\")\n",
    "    print(aggregated_df.head(10).to_string(index=False))\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4091d13",
   "metadata": {},
   "source": [
    "## Step 3: Verify No Duplicates After Merge\n",
    "\n",
    "**Verification**: Before merging datasets, verify that all (gw, name) pairs are now unique in both datasets.\n",
    "\n",
    "This is important because (gw, name) will be used as the unique identifier for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fff8345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DUPLICATE (GW, NAME) VALIDATION CHECK\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Checking: gw_merged_filtered_nodup.csv\n",
      "File: gw_vaa/gw_merged_filtered_nodup.csv\n",
      "================================================================================\n",
      "\n",
      "Total rows: 27,231\n",
      "\n",
      "✓ PASS: No duplicate (gw, name) combinations found!\n",
      "  Total unique (gw, name) combinations: 27,231\n",
      "\n",
      "================================================================================\n",
      "Checking: playerstats_mapped_filtered.csv\n",
      "File: gw_olb/playerstats_mapped_filtered.csv\n",
      "================================================================================\n",
      "\n",
      "Total rows: 27,657\n",
      "\n",
      "✓ PASS: No duplicate (gw, name) combinations found!\n",
      "  Total unique (gw, name) combinations: 27,657\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "gw_merged_filtered_nodup.csv:    PASS ✓\n",
      "playerstats_mapped_filtered.csv: PASS ✓\n",
      "\n",
      "================================================================================\n",
      "✓ ALL CHECKS PASSED - Both files have no duplicate (gw, name) combinations\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates_both(file1_path, file2_path):\n",
    "    \"\"\"Check both files for duplicate (gw, name) pairs.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DUPLICATE (GW, NAME) VALIDATION CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check VAA file\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Checking: gw_merged_filtered_nodup.csv\")\n",
    "    print(f\"File: {file1_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df1 = pd.read_csv(file1_path)\n",
    "    total_rows1 = len(df1)\n",
    "    print(f\"\\nTotal rows: {total_rows1:,}\")\n",
    "    \n",
    "    duplicates1 = df1.groupby(['gw', 'name']).size().reset_index(name='count')\n",
    "    duplicates1 = duplicates1[duplicates1['count'] > 1].sort_values('count', ascending=False)\n",
    "    \n",
    "    if len(duplicates1) == 0:\n",
    "        print(f\"\\n✓ PASS: No duplicate (gw, name) combinations found!\")\n",
    "        unique_combinations1 = len(df1.groupby(['gw', 'name']))\n",
    "        print(f\"  Total unique (gw, name) combinations: {unique_combinations1:,}\")\n",
    "        file1_ok = True\n",
    "    else:\n",
    "        print(f\"\\n✗ FAIL: Found {len(duplicates1)} duplicate (gw, name) combinations!\")\n",
    "        file1_ok = False\n",
    "    \n",
    "    # Check OLB file\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Checking: playerstats_mapped_filtered.csv\")\n",
    "    print(f\"File: {file2_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df2 = pd.read_csv(file2_path)\n",
    "    total_rows2 = len(df2)\n",
    "    print(f\"\\nTotal rows: {total_rows2:,}\")\n",
    "    \n",
    "    duplicates2 = df2.groupby(['gw', 'player_name']).size().reset_index(name='count')\n",
    "    duplicates2 = duplicates2[duplicates2['count'] > 1].sort_values('count', ascending=False)\n",
    "    \n",
    "    if len(duplicates2) == 0:\n",
    "        print(f\"\\n✓ PASS: No duplicate (gw, name) combinations found!\")\n",
    "        unique_combinations2 = len(df2.groupby(['gw', 'player_name']))\n",
    "        print(f\"  Total unique (gw, name) combinations: {unique_combinations2:,}\")\n",
    "        file2_ok = True\n",
    "    else:\n",
    "        print(f\"\\n✗ FAIL: Found {len(duplicates2)} duplicate (gw, name) combinations!\")\n",
    "        file2_ok = False\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\ngw_merged_filtered_nodup.csv:    {'PASS ✓' if file1_ok else 'FAIL ✗'}\")\n",
    "    print(f\"playerstats_mapped_filtered.csv: {'PASS ✓' if file2_ok else 'FAIL ✗'}\")\n",
    "    \n",
    "    if file1_ok and file2_ok:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"✓ ALL CHECKS PASSED - Both files have no duplicate (gw, name) combinations\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"✗ VALIDATION FAILED - Please review the duplicates listed above\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "check_duplicates_both(\n",
    "    'gw_vaa/gw_merged_filtered_nodup.csv',\n",
    "    'gw_olb/playerstats_mapped_filtered.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355668c",
   "metadata": {},
   "source": [
    "## Step 4: Match Datasets by (name, gw) Pairs\n",
    "\n",
    "**Problem**: There are 27,658 - 27,231 = **427 row difference** between the two datasets.\n",
    "\n",
    "**Investigation**: Match the two datasets by (name, gw) pair and identify which pairs exist in only one dataset.\n",
    "\n",
    "**Expected Results** (from problem sheet):\n",
    "- Total unique (name, gw) pairs in OLB dataset: 27,657\n",
    "- Total unique (name, gw) pairs in VAA dataset: 27,231\n",
    "- Matching pairs (in both datasets): 27,222\n",
    "- Match percentage: **98.43%**\n",
    "- Pairs only in OLB dataset: 435\n",
    "- Pairs only in VAA dataset: 9\n",
    "\n",
    "**Decision**: Since both datasets complement each other and there's a 98.43% match percentage (quite good for real-life datasets), we'll keep only the matching pairs for integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5377e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "OLB data loaded: 27657 rows\n",
      "VAA data loaded: 27231 rows\n",
      "================================================================================\n",
      "DATASET MATCHING REPORT\n",
      "================================================================================\n",
      "\n",
      "SUMMARY STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Total unique (name, gw) pairs in OLB dataset: 27657\n",
      "Total unique (name, gw) pairs in VAA dataset: 27231\n",
      "Matching pairs (in both datasets):           27222\n",
      "Match percentage:                             98.43%\n",
      "\n",
      "Pairs only in OLB dataset:                    435\n",
      "Pairs only in VAA dataset:                    9\n",
      "\n",
      "================================================================================\n",
      "NON-MATCHING PAIRS BY GAMEWEEK\n",
      "================================================================================\n",
      "\n",
      "PAIRS ONLY IN OLB (by GW):\n",
      "--------------------------------------------------------------------------------\n",
      "  GW 34: 155 pairs\n",
      "  GW 29: 151 pairs\n",
      "  GW 15:  63 pairs\n",
      "  GW 22:  26 pairs\n",
      "  GW 24:  12 pairs\n",
      "  GW 20:  10 pairs\n",
      "  GW  3:   7 pairs\n",
      "  GW 21:   2 pairs\n",
      "  GW 23:   2 pairs\n",
      "  GW 27:   2 pairs\n",
      "  GW 25:   1 pairs\n",
      "  GW 26:   1 pairs\n",
      "  GW 28:   1 pairs\n",
      "  GW 30:   1 pairs\n",
      "  GW 31:   1 pairs\n",
      "\n",
      "PAIRS ONLY IN VAA (by GW):\n",
      "--------------------------------------------------------------------------------\n",
      "  GW 23:   1 pairs\n",
      "  GW 24:   1 pairs\n",
      "  GW 25:   1 pairs\n",
      "  GW 26:   1 pairs\n",
      "  GW 27:   1 pairs\n",
      "  GW 28:   1 pairs\n",
      "  GW 29:   1 pairs\n",
      "  GW 30:   1 pairs\n",
      "  GW 31:   1 pairs\n",
      "\n",
      "================================================================================\n",
      "END OF REPORT\n",
      "================================================================================\n",
      "\n",
      "Report saved to: dataset_matching_report.txt\n"
     ]
    }
   ],
   "source": [
    "def match_datasets():\n",
    "    \"\"\"\n",
    "    Match two datasets by (name, gw) pairs and identify non-matching pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load only the necessary columns from each dataset\n",
    "    print(\"Loading datasets...\")\n",
    "    olb_data = pd.read_csv('gw_olb/playerstats_mapped_filtered.csv', usecols=['player_name', 'gw'])\n",
    "    vaa_data = pd.read_csv('gw_vaa/gw_merged_filtered_nodup.csv', usecols=['name', 'gw'])\n",
    "    \n",
    "    print(f\"OLB data loaded: {len(olb_data)} rows\")\n",
    "    print(f\"VAA data loaded: {len(vaa_data)} rows\")\n",
    "    \n",
    "    # Create sets of (name, gw) tuples\n",
    "    olb_pairs = set(zip(olb_data['player_name'], olb_data['gw']))\n",
    "    vaa_pairs = set(zip(vaa_data['name'], vaa_data['gw']))\n",
    "    \n",
    "    # Find non-matching pairs\n",
    "    only_in_olb = olb_pairs - vaa_pairs\n",
    "    only_in_vaa = vaa_pairs - olb_pairs\n",
    "    matching_pairs = olb_pairs & vaa_pairs\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_olb = len(olb_pairs)\n",
    "    total_vaa = len(vaa_pairs)\n",
    "    total_matching = len(matching_pairs)\n",
    "    match_percentage = (total_matching / max(total_olb, total_vaa)) * 100\n",
    "    \n",
    "    # Prepare report\n",
    "    report_lines = []\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(\"DATASET MATCHING REPORT\")\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"SUMMARY STATISTICS\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    report_lines.append(f\"Total unique (name, gw) pairs in OLB dataset: {total_olb}\")\n",
    "    report_lines.append(f\"Total unique (name, gw) pairs in VAA dataset: {total_vaa}\")\n",
    "    report_lines.append(f\"Matching pairs (in both datasets):           {total_matching}\")\n",
    "    report_lines.append(f\"Match percentage:                             {match_percentage:.2f}%\")\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(f\"Pairs only in OLB dataset:                    {len(only_in_olb)}\")\n",
    "    report_lines.append(f\"Pairs only in VAA dataset:                    {len(only_in_vaa)}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Add gameweek-level analysis\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(\"NON-MATCHING PAIRS BY GAMEWEEK\")\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    if only_in_olb:\n",
    "        # Count pairs by gameweek for OLB\n",
    "        gw_counts_olb = {}\n",
    "        for name, gw in only_in_olb:\n",
    "            gw_counts_olb[gw] = gw_counts_olb.get(gw, 0) + 1\n",
    "        \n",
    "        report_lines.append(\"PAIRS ONLY IN OLB (by GW):\")\n",
    "        report_lines.append(\"-\" * 80)\n",
    "        # Sort by count descending\n",
    "        for gw, count in sorted(gw_counts_olb.items(), key=lambda x: (-x[1], x[0])):\n",
    "            report_lines.append(f\"  GW {gw:2d}: {count:3d} pairs\")\n",
    "        report_lines.append(\"\")\n",
    "    \n",
    "    if only_in_vaa:\n",
    "        # Count pairs by gameweek for VAA\n",
    "        gw_counts_vaa = {}\n",
    "        for name, gw in only_in_vaa:\n",
    "            gw_counts_vaa[gw] = gw_counts_vaa.get(gw, 0) + 1\n",
    "        \n",
    "        report_lines.append(\"PAIRS ONLY IN VAA (by GW):\")\n",
    "        report_lines.append(\"-\" * 80)\n",
    "        # Sort by count descending\n",
    "        for gw, count in sorted(gw_counts_vaa.items(), key=lambda x: (-x[1], x[0])):\n",
    "            report_lines.append(f\"  GW {gw:2d}: {count:3d} pairs\")\n",
    "        report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(\"END OF REPORT\")\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    \n",
    "    # Print to console\n",
    "    report_text = \"\\n\".join(report_lines)\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = 'dataset_matching_report.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"\\nReport saved to: {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'total_olb': total_olb,\n",
    "        'total_vaa': total_vaa,\n",
    "        'matching': total_matching,\n",
    "        'only_in_olb': len(only_in_olb),\n",
    "        'only_in_vaa': len(only_in_vaa),\n",
    "        'match_percentage': match_percentage\n",
    "    }\n",
    "\n",
    "results = match_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f472a2",
   "metadata": {},
   "source": [
    "## Step 5: Integrate Datasets\n",
    "\n",
    "**Final Step**: Filter both datasets to keep only rows where (name, gw) exists in matching pairs, then merge them.\n",
    "\n",
    "**Process**:\n",
    "1. OLB: Keep only rows where (player_name, gw) is in matching pairs\n",
    "2. VAA: Keep only rows where (name, gw) is in matching pairs\n",
    "3. Merge on (name, gw)\n",
    "4. Save as `integrated_df.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4584d5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "OLB data loaded: 27657 rows, 9 columns\n",
      "VAA data loaded: 27231 rows, 9 columns\n",
      "\n",
      "Identifying matching pairs...\n",
      "Total matching pairs: 27222\n",
      "Pairs to remove from OLB: 435\n",
      "Pairs to remove from VAA: 9\n",
      "\n",
      "Filtering OLB dataset...\n",
      "OLB filtered: 27222 rows remaining\n",
      "Filtering VAA dataset...\n",
      "VAA filtered: 27222 rows remaining\n",
      "\n",
      "Merging datasets...\n",
      "Integrated dataset: 27222 rows, 16 columns\n",
      "\n",
      "Column names in integrated dataset:\n",
      "['name', 'status', 'chance_of_playing_next_round', 'chance_of_playing_this_round', 'now_cost', 'event_points', 'ep_next', 'ep_this', 'gw', 'position', 'team', 'xP', 'minutes', 'starts', 'total_points', 'value']\n",
      "\n",
      "✓ Integrated dataset saved to: integrated_df.csv\n",
      "\n",
      "================================================================================\n",
      "INTEGRATION SUMMARY\n",
      "================================================================================\n",
      "Original OLB rows:        27657\n",
      "Original VAA rows:        27231\n",
      "Rows removed from OLB:    435\n",
      "Rows removed from VAA:    9\n",
      "Final integrated rows:    27222\n",
      "Total columns:            16\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def integrate_datasets():\n",
    "    \"\"\"\n",
    "    Filter and integrate OLB and VAA datasets based on matching (name, gw) pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    # Load both datasets\n",
    "    olb_data = pd.read_csv('gw_olb/playerstats_mapped_filtered.csv')\n",
    "    vaa_data = pd.read_csv('gw_vaa/gw_merged_filtered_nodup.csv')\n",
    "    \n",
    "    print(f\"OLB data loaded: {len(olb_data)} rows, {len(olb_data.columns)} columns\")\n",
    "    print(f\"VAA data loaded: {len(vaa_data)} rows, {len(vaa_data.columns)} columns\")\n",
    "    \n",
    "    # Create sets of (name, gw) tuples\n",
    "    print(\"\\nIdentifying matching pairs...\")\n",
    "    olb_pairs = set(zip(olb_data['player_name'], olb_data['gw']))\n",
    "    vaa_pairs = set(zip(vaa_data['name'], vaa_data['gw']))\n",
    "    \n",
    "    # Find matching pairs (intersection)\n",
    "    matching_pairs = olb_pairs & vaa_pairs\n",
    "    \n",
    "    print(f\"Total matching pairs: {len(matching_pairs)}\")\n",
    "    print(f\"Pairs to remove from OLB: {len(olb_pairs) - len(matching_pairs)}\")\n",
    "    print(f\"Pairs to remove from VAA: {len(vaa_pairs) - len(matching_pairs)}\")\n",
    "    \n",
    "    # Filter OLB dataset to keep only matching pairs\n",
    "    print(\"\\nFiltering OLB dataset...\")\n",
    "    olb_filtered = olb_data[\n",
    "        olb_data.apply(lambda row: (row['player_name'], row['gw']) in matching_pairs, axis=1)\n",
    "    ].copy()\n",
    "    \n",
    "    # Rename player_name to name in OLB dataset\n",
    "    olb_filtered.rename(columns={'player_name': 'name'}, inplace=True)\n",
    "    \n",
    "    print(f\"OLB filtered: {len(olb_filtered)} rows remaining\")\n",
    "    \n",
    "    # Filter VAA dataset to keep only matching pairs\n",
    "    print(\"Filtering VAA dataset...\")\n",
    "    vaa_filtered = vaa_data[\n",
    "        vaa_data.apply(lambda row: (row['name'], row['gw']) in matching_pairs, axis=1)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"VAA filtered: {len(vaa_filtered)} rows remaining\")\n",
    "    \n",
    "    # Merge the two datasets on (name, gw)\n",
    "    print(\"\\nMerging datasets...\")\n",
    "    integrated_df = pd.merge(\n",
    "        olb_filtered,\n",
    "        vaa_filtered,\n",
    "        on=['name', 'gw'],\n",
    "        how='inner',\n",
    "        suffixes=('_olb', '_vaa')\n",
    "    )\n",
    "    \n",
    "    print(f\"Integrated dataset: {len(integrated_df)} rows, {len(integrated_df.columns)} columns\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(\"\\nColumn names in integrated dataset:\")\n",
    "    print(integrated_df.columns.tolist())\n",
    "    \n",
    "    # Save the integrated dataset\n",
    "    output_file = 'integrated_df.csv'\n",
    "    integrated_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Integrated dataset saved to: {output_file}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INTEGRATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Original OLB rows:        {len(olb_data)}\")\n",
    "    print(f\"Original VAA rows:        {len(vaa_data)}\")\n",
    "    print(f\"Rows removed from OLB:    {len(olb_data) - len(olb_filtered)}\")\n",
    "    print(f\"Rows removed from VAA:    {len(vaa_data) - len(vaa_filtered)}\")\n",
    "    print(f\"Final integrated rows:    {len(integrated_df)}\")\n",
    "    print(f\"Total columns:            {len(integrated_df.columns)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return integrated_df\n",
    "\n",
    "integrated_df = integrate_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c1057",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Integration Complete!\n",
    "\n",
    "The integrated dataset has been successfully created as `integrated_df.csv`.\n",
    "\n",
    "This dataset combines:\n",
    "- **OLB data**: status, chance_of_playing_next_round, chance_of_playing_this_round, now_cost, event_points, ep_next, ep_this\n",
    "- **VAA data**: position, team, xP, minutes, starts, total_points, value\n",
    "\n",
    "All rows represent unique (name, gw) pairs that exist in both original datasets, ensuring data quality and consistency.\n",
    "\n",
    "**Next Steps**: The integrated data can now be cleaned and processed in the `data_cleaning.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
