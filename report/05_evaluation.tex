\section{Evaluation}


\subsection{Evaluation Methodology}

The optimization system underwent comprehensive backtesting across the full 2024-25 Fantasy Premier League season (38 gameweeks). Evaluation encompassed two prediction scenarios:

\begin{itemize}
    \item \textbf{Oracle scenario}: Employs actual gameweek outcomes as predictions, simulating perfect foresight. This validates algorithmic correctness and delineates an upper performance bound, yielding particularly insightful analytical benchmarks.
    \item \textbf{Normal scenario}: Leverages expected points from integrated sources (calibrated per the implementation section). This assesses the pipeline's efficacy with external predictions.
\end{itemize}

Performance was quantified via four metrics: seasonal total points, GW1 points (gauging initial squad efficacy), total transfers executed, and average solve time per instance.

\subsection{Experimental Setup}

Evaluation encompassed six solver configurations: three MILP solvers (CBC, GLPK, SCIP) via PuLP, and three CP solvers (CP-SAT, Chuffed, Gecode) via MiniZinc. MILP instances resolved expeditiously. CP solvers were assessed under two timeout regimes—10 seconds and 60 seconds—to quantify computational tradeoffs.

The oracle scenario spanned all six solvers, facilitating algorithmic comparisons under perfect foresight. The normal scenario was confined to MILP solvers, as CP variants yielded impractical runtimes despite extended time limits. Benchmarking against real FPL manager rankings drew from the FPL community website, a reliable source given the prominence of top performers within the community.


\subsection{Results}

Table~\ref{tab:solver_performance} presents the comprehensive performance comparison across all solver configurations and scenarios.

\begin{table}[h]
\centering
\small
\begin{tabular}{llccrrr}
\toprule
\textbf{Approach} & \textbf{Solver} & \textbf{Scenario} & \textbf{GW1} & \textbf{Total} & \textbf{Transfers} & \textbf{Avg Time} \\
 &  &  & \textbf{Points} & \textbf{Points} &  & \textbf{(sec)} \\
\midrule
MILP & CBC & Oracle & 131 & 5548 & 280 & 0.20 \\
MILP & CBC & Normal & 71 & 3152 & 91 & 0.14 \\
MILP & GLPK & Oracle & 131 & 5579 & 287 & 0.12 \\
MILP & GLPK & Normal & 62 & 3206 & 88 & 0.14 \\
MILP & SCIP & Oracle & 131 & 5597 & 291 & 0.20 \\
MILP & SCIP & Normal & 76 & 3232 & 92 & 0.24 \\
\midrule
CP & CP-SAT & Oracle (10s) & 99 & 3417 & 145 & $\sim$10.0$^*$ \\
CP & CP-SAT & Oracle (60s) & 131 & 3911 & 178 & $\sim$60.0$^{**}$ \\
CP & Chuffed & Oracle (10s) & 56 & 2291 & 75 & $\sim$10.0$^*$ \\
CP & Chuffed & Oracle (60s) & 74 & 2531 & 102 & $\sim$60.0$^*$ \\
CP & Gecode & Oracle (10s) & 77 & 2732 & 74 & $\sim$10.0$^*$ \\
CP & Gecode & Oracle (60s) & 83 & 2935 & 91 & $\sim$60.0$^*$ \\
\bottomrule
\multicolumn{7}{l}{\footnotesize $^*$Solver reached timeout limit on most gameweeks} \\
\multicolumn{7}{l}{\footnotesize $^{**}$CP-SAT achieved optimal solution for GW1 in 57 seconds, then hit timeout on later gameweeks}
\end{tabular}
\caption{Performance comparison across all solver configurations. Oracle scenario uses perfect foresight of actual gameweek outcomes; Normal scenario uses expected points from integrated data sources.}
\label{tab:solver_performance}
\end{table}

\subsection{Results Interpretation}

To contextualize these results, actual FPL manager rankings from 2024-25 indicate 
the top-ranked manager amassed 2810 points, with rank 10 at 2754, rank 1000 at 2657, 
and rank 10,000 at 2599. In the normal scenario, MILP solvers yielded substantially 
superior totals (CBC: 3152; GLPK: 3206; SCIP: 3232 points), surpassing elite human 
benchmarks. This discrepancy implies overfitting in expected points data, likely 
incorporating post-deadline information unavailable during real-time decision-making. 
Oracle outcomes (CBC: 5548; GLPK: 5579; SCIP: 5597 points) approximate double top 
human performance.

The oracle scenario affirms algorithmic fidelity: with flawless predictions, MILP 
solvers consistently deliver optimal or near-optimal solutions, varying by at most 
1\%. Multiple optima per gameweek—especially in bench selection—engender subtle 
divergences propagating as butterfly effects through subsequent gameweeks, accounting 
for inter-solver point variances despite individual optimality.

\subsection{Comparative Analysis}

\subsubsection{MILP vs CP Performance}

In the oracle scenario, the leading MILP solver (SCIP: 5597 points) surpassed the 
top CP solver (CP-SAT with 60-second timeout: 3911 points) by 1686 points. MILP 
solvers attained these outcomes in under 0.25 seconds average, whereas CP solvers 
exhausted 60-second timeouts without consistently reaching optimality. CP-SAT 
resolved GW1 optimally in 57 seconds, illustrating theoretical parity with 
unconstrained time, but at 240 times average MILP runtime (0.25 seconds), CP 
performance renders it impractical for real-time applications, justifying the 
decision to forego hybrid MILP/CP decomposition.

\subsubsection{Transfer Behavior}

Transfer patterns illustrate prediction quality influence on optimization strategy. 
In oracle scenarios, MILP solvers executed 280-291 transfers (averaging 7.4-7.7 
per week) versus 88-92 transfers (averaging 2.3-2.4 per week) in normal scenarios. 
Under perfect foresight, expected point gains from weekly adjustments routinely 
surpass the 4-point penalty, fostering aggressive transfers. Conversely, conservative 
expected points in normal scenarios yield marginal or negative net gains for excess 
transfers, promoting restraint. The optimization adaptively responds to input 
prediction magnitudes: substantial differentials justify additional transfers, 
while subdued ones favor restraint.

\subsection{Success Analysis}

The evaluation demonstrates three key successes. The oracle scenario's attainment 
of roughly double the points of the top human manager demonstrates that optimization 
does not constitute a bottleneck, enabling substantial performance gains under 
perfect foresight. MILP solvers resolved instances in under 0.25 seconds consistently, 
supporting real-time deployment in interactive settings and underscoring the 
hierarchical decomposition and transfer dynamics modeling tractability.

\subsection{Limitations and Shortcomings}

Despite its strengths, the project exhibits significant limitations. CP solvers 
encountered severe timeouts, routinely exhausting 10-60 second limits without 
completing searches. Although CP-SAT resolved GW1 optimally in 57 seconds—indicating 
theoretical feasibility—this latency proves untenable for real-time applications 
demanding sub-second responses, rendering CP formulations unsuitable for production 
environments.

Multiple optimal solutions exist in each gameweek, particularly for bench selection. 
Although bench players do not influence the objective function, they impact budget 
allocation and subsequent transfer options. Consequently, minor bench composition 
differences propagate as cascading effects through later gameweeks, accounting for 
subtle variations in season-long point totals across solvers, each identifying 
optimal solutions per gameweek. Absent simultaneous multi-week modeling, such 
trajectory divergences remain inherent.