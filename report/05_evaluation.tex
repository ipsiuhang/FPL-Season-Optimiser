\section{Evaluation}


\subsection{Evaluation Methodology}

The optimization system underwent comprehensive backtesting across the full 2024-25 Fantasy Premier League season (38 gameweeks). Evaluation encompassed two prediction scenarios:

\begin{itemize}
    \item \textbf{Oracle scenario}: Employs actual gameweek outcomes as predictions, simulating perfect foresight. This validates algorithmic correctness and delineates an upper performance bound, yielding particularly insightful analytical benchmarks.
    \item \textbf{Normal scenario}: Leverages expected points from integrated sources (calibrated per the implementation section). This assesses the pipeline's efficacy with external predictions.
\end{itemize}

Performance was quantified via four metrics: seasonal total points, GW1 points (gauging initial squad efficacy), total transfers executed, and average solve time per instance.

\subsection{Experimental Setup}

Evaluation encompassed six solver configurations: three MILP solvers (CBC, GLPK, SCIP) via PuLP, and three CP solvers (CP-SAT, Chuffed, Gecode) via MiniZinc. MILP instances resolved expeditiously. CP solvers were assessed under two timeout regimes—10 seconds and 60 seconds—to quantify computational tradeoffs.

The oracle scenario spanned all six solvers, facilitating algorithmic comparisons under perfect foresight. The normal scenario was confined to MILP solvers, as CP variants yielded impractical runtimes despite extended time limits. Benchmarking against real FPL manager rankings drew from the FPL community website, a reliable source given the prominence of top performers within the community.


\subsection{Results}

Table~\ref{tab:solver_performance} presents the comprehensive performance comparison across all solver configurations and scenarios.

\begin{table}[h]
\centering
\small
\begin{tabular}{llccrrr}
\toprule
\textbf{Approach} & \textbf{Solver} & \textbf{Scenario} & \textbf{GW1} & \textbf{Total} & \textbf{Transfers} & \textbf{Avg Time} \\
 &  &  & \textbf{Points} & \textbf{Points} &  & \textbf{(sec)} \\
\midrule
MILP & CBC & Oracle & 131 & 5548 & 280 & 0.20 \\
MILP & CBC & Normal & 71 & 3152 & 91 & 0.14 \\
MILP & GLPK & Oracle & 131 & 5579 & 287 & 0.12 \\
MILP & GLPK & Normal & 62 & 3206 & 88 & 0.14 \\
MILP & SCIP & Oracle & 131 & 5597 & 291 & 0.20 \\
MILP & SCIP & Normal & 76 & 3232 & 92 & 0.24 \\
\midrule
CP & CP-SAT & Oracle (10s) & 99 & 3417 & 145 & $\sim$10.0$^*$ \\
CP & CP-SAT & Oracle (60s) & 131 & 3911 & 178 & $\sim$60.0$^{**}$ \\
CP & Chuffed & Oracle (10s) & 56 & 2291 & 75 & $\sim$10.0$^*$ \\
CP & Chuffed & Oracle (60s) & 74 & 2531 & 102 & $\sim$60.0$^*$ \\
CP & Gecode & Oracle (10s) & 77 & 2732 & 74 & $\sim$10.0$^*$ \\
CP & Gecode & Oracle (60s) & 83 & 2935 & 91 & $\sim$60.0$^*$ \\
\bottomrule
\multicolumn{7}{l}{\footnotesize $^*$Solver reached timeout limit on most gameweeks} \\
\multicolumn{7}{l}{\footnotesize $^{**}$CP-SAT achieved optimal solution for GW1 in 57 seconds, then hit timeout on later gameweeks}
\end{tabular}
\caption{Performance comparison across all solver configurations. Oracle scenario uses perfect foresight of actual gameweek outcomes; Normal scenario uses expected points from integrated data sources.}
\label{tab:solver_performance}
\end{table}

\subsection{Results Interpretation}

To contextualize these results, actual FPL manager rankings from the 2024-25 season (sourced from the official FPL community website) indicate that the top-ranked manager amassed 2810 points, with rank 10 at 2754, rank 1000 at 2657, and rank 10,000 at 2599. In the normal scenario, MILP solvers yielded substantially superior totals (CBC: 3152; GLPK: 3206; SCIP: 3232 points), surpassing the elite human benchmark. This discrepancy implies overfitting in the expected points data—likely incorporating post-deadline information unavailable in real-time decision-making. Oracle outcomes (CBC: 5548; GLPK: 5579; SCIP: 5597 points) approximate double the top human performance.

The oracle scenario affirms algorithmic fidelity: with flawless predictions, all three MILP solvers consistently deliver optimal or near-optimal solutions, varying by at most 1\% from one another. Notably, multiple optima per gameweek—especially in bench selection, which indirectly influences budgets and transfers—engender subtle divergences. These propagate as butterfly effects through subsequent gameweeks, accounting for inter-solver point variances despite individual optimality.

\subsection{Comparative Analysis}

\subsubsection{MILP vs CP Performance}

In the oracle scenario, the leading MILP solver (SCIP: 5597 points) surpassed the top CP solver (CP-SAT with 60-second timeout: 3911 points) by 1686 points. MILP solvers attained these outcomes in under 0.25 seconds on average, whereas CP solvers exhausted the 60-second timeouts without consistently reaching optimality.

CP-SAT resolved GW1 optimally in 57 seconds, illustrating that CP solvers could theoretically achieve full optimality with unconstrained time. Nonetheless, at 240 times the average MILP runtime (0.25 seconds per gameweek), CP's performance renders it impractical for real-time applications. This disparity justifies forgoing the initially proposed hybrid MILP/CP decomposition.

\subsubsection{Transfer Behavior}

Transfer patterns illustrate the influence of prediction quality on optimization strategy. In the oracle scenario, MILP solvers executed 280--291 transfers (averaging 7.4--7.7 per week), compared to 88--92 transfers (averaging 2.3--2.4 per week) in the normal scenario. This disparity stems from prediction scale and confidence, rather than accuracy alone.

Under perfect foresight (oracle), expected point gains from weekly squad adjustments routinely surpass the 4-point transfer penalty, fostering aggressive transfers. Conversely, the conservative expected points in the normal scenario yield marginal or negative net gains for excess transfers, promoting preservation of free allowances. Thus, the optimization adaptively responds to input prediction magnitudes: substantial differentials justify additional transfers, while subdued ones favor restraint.

For CP solvers, transfer counts (74-178) cannot be meaningfully interpreted as the search was incomplete within the timeout limits.


\subsection{Success Analysis}

The evaluation demonstrates three key successes:

\textbf{Practical Modeling} The oracle scenario's attainment of roughly double the points of the top human manager demonstrates that the optimization problem does not constitute a bottleneck in the FPL game, enabling substantial performance gains under perfect foresight.

\textbf{Computational Efficiency} MILP solvers resolved instances in under 0.25 seconds consistently, supporting real-time deployment in interactive settings. This performance underscores the tractability of the hierarchical decomposition and transfer dynamics modeling.

\subsection{Limitations and Shortcomings}

\textbf{Constraint Programming Timeout Issues} CP solvers encountered severe timeouts, routinely exhausting the 10--60 second limits without completing searches. Although CP-SAT resolved GW1 optimally in 57 seconds—indicating theoretical feasibility, this latency is untenable for real-time applications demanding sub-second responses. At 240 times MILP's average speed, CP formulations prove unsuitable for production environments.


\textbf{Multiple Optima Effects} Multiple optimal solutions exist in each gameweek's optimization, particularly for bench selection. Although bench players do not influence the objective function, they impact budget allocation and subsequent transfer options. Consequently, minor differences in bench composition propagate as cascading effects through later gameweeks. This accounts for the subtle variations in season-long point totals across solvers, each of which identifies optimal solutions on a per-gameweek basis. Absent simultaneous multi-week modeling, such trajectory divergences remain inherent.
